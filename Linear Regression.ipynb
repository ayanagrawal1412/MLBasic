{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Math\n",
    "\n",
    "|Symbol| Meaning|\n",
    "|--|--|\n",
    "|$x$ | feature|\n",
    "|$y$ | target|\n",
    "|$w$ | slope for target vs feature graph or weight for single variable Linear Regression|\n",
    "|$b$ | intercept for target vs feature graph or bias|\n",
    "|$x^{(i)}$ | i<sup>th</sup> training example in case of Single Variable Linear Regression|\n",
    "|$\\hat y$ | estimated value of target|\n",
    "|$m$|No. of training examples|\n",
    "|$n$|No. of features in a single training example|\n",
    "|$x^{(i)}_j$ | j<sup>th</sup> feature of i<sup>th</sup> training example for Multiple Variable Linear Regression|\n",
    "\n",
    "Note on notations: Capital bold face used to denote Matrix (or 2d arrays); small bold face symbol to denote a vector (or 1d array) and when used in matrix equations, can be used to denote a single column matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Model Representation** (sometimes called a hypothesis function):\n",
    "\n",
    "For a single featured training set:\n",
    "\\begin{align*} f(x^{(i)}) &= \\hat {y^{(i)}} \\\\\n",
    "f_{wb}(x^{(i)}) &= wx^{(i)} + b \\tag{1} \\end{align*}\n",
    "\n",
    "**Mean Squared Error (MSE)** Cost\n",
    "\\begin{align*}J &= \\sum_{i=1}^{m} \\frac{(f(x^{(i)})-y^{(i)})^2}{2m} \\tag{2} \\\\\n",
    "J_{wb} &= \\sum_{i=1}^{m} \\frac{(wx^{(i)} + b-y^{(i)})^2}{2m} \\tag{3} \\end{align*}\n",
    "\n",
    "To find $w$ and $b$, and thus the estimator function we must minimize cost ( $J$ ) wrt $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuition for Mean Squared Error Cost function consider this - to approximate a bunch of numbers by a single number we often cite the data's average or arithmetic mean.\n",
    "\n",
    "We could use median too but it takes more effort to calculate median as it requires sorting the numbers and then picking the middle one. But in some cases median is more suited and we must pick median over mean. (Example- salaries of individuals in a population are often skewed to the right and median is more apt to report which confirms to us that atleast half the population is earning more than the reported number - 'median' and the other half is earning less than this. With mean we can't make such statements, all we can say that total salary divided by total number of individuals is whats being reported as mean.)\n",
    "\n",
    "So now getting back to mean:\n",
    "\n",
    "**Mean is the point from which the arithmetic sum of differences from all the numbers is zero.**\n",
    "\n",
    "Let $\\bar x$ denote the mean of a set of scalar $x^{(i)}$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} (x^{(i)}-\\bar x) = \\sum_{i=1}^{m}x^{(i)} - m\\bar x = \\sum_{i=1}^{m}x^{(i)}- \\sum_{i=1}^{m}x^{(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So since the mean has this nice property that total deviations from the mean is zero which also gives us some motivation to use it to summarize - basically if we use mean our total deviations or algebraic sum of errors go to zero. But the same property inhibits us from using the sum of deviations from the mean as a measure of variability within the data.\n",
    "\n",
    "What we could do is take the absoulte deviations. (We'll see this later)\n",
    "\n",
    "The other option is to square the deviations so as to get rid of the signs of the deviations and then sum it. A measure obtained by doing exactly this, is whats called **variance** specifically - the **Mean of Squared Deviations** or **MSD**. Note that for variance calculation the deviations are always considered from the mean.\n",
    "\n",
    "Now since variance has the units which are square of the original quantity, we often take the square root of variance to get standard deviation which is essentially Root of Mean Squared Deviations (**RMSD**).\n",
    "\n",
    "What if we consider the squared deviations from any other number - say median or just some other number, we'll see below that in that case, the total deviations would be larger than that taken from mean. That is **sum of squared deviations is minimum when deviations are considered from or around mean**\n",
    "\n",
    "If we were to find a number, lets say $\\mu$, sum of squares of differences from which of all the numbers in a dataset, is to be minimized, we'd see that that number, is in fact, the mean. For this derivation, $\\mu$ is just some unknown number and not mean, ($\\bar x$ is what we use for mean here). But the derivation would prove that $\\mu$ would turn out to be same as $\\bar x$ or mean.\n",
    "\n",
    "To minimize $$ \\sum_{i=1}^{m} (x^{(i)}-\\mu)^2$$\n",
    "lets take the derivative of the above term wrt $\\mu$ and equate it to zero.\n",
    "\\begin{align*}\\dfrac{d}{d \\mu} \\left(\\sum_{i=1}^{m} (x^{(i)}-\\mu)^2\\right)=0 \\\\\n",
    "\\sum_{i=1}^{m} \\left(2(x^{(i)}-\\mu) \\cdot \\frac{d(x^{(i)}-\\mu)}{d \\mu}\\right) = 0 \\\\\n",
    "-2 \\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\Rightarrow \\mu = \\bar x\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Thus (arithmetic) mean is the point, sum of squares of differences from which of all the numbers in a dataset, is minimum.**\n",
    "\n",
    "So we've established that mean is useful for summarizing a bunch of data points by a single number. Or that if we see a bunch of data points and if we're asked to predict or estimate the next number then the best estimate we could give is mean (need to invoke a bit of probability theory to properly substantiate the last statement).\n",
    "\n",
    "But lets turn our attention back to deviations, we said for measuring the variability in the data, we could just add the absolute values of deviations instead of the squares of deviations. Total Absolute Deviations divided by total number of data points gives us **Mean Absolute Deviation** or **MAD**.\n",
    "\n",
    "So just a few lines back we said that squared deviations is minimum when the deviations are computed from the mean. Can we say the same for absolute deviations too - *NO*. So it seems if we were to find a number from which sum of absolute deviations is minimum then it would turn out to be the median of those numbers, thus\n",
    "\n",
    "**Median is the point, sum of absolute deviations (or distances) from which of all the numbers in a dataset is minimum.**\n",
    "\n",
    "Thus we can say it would make more sense to work with MAD and median together, just like we work with MSD and mean together. We want to give the lowest figure for variability, ie we mostly desire less variability, and MAD gives the lowest figure when deviations are considered around median, whereas MSD gives the lowest figure when deviations are considered around mean. Both MAD and MSD are measures of variability whereas mean and median are measures of where the data points are centered. As mentioned before, sometimes we use RMSD instead of MSD so that the units of the quantities all match with each other and it makes some expressions look neater.\n",
    "\n",
    "But just as median is a bit more difficult to work with than mean, Mean Absolute Differences is a bit more difficult to work with than Mean Squared Deviations (which is what Variance is if the deviations are taken from mean). One intuition of why MAD is difficult to work with is that the absolute function graph is non-differentiable at zero.\n",
    "\n",
    "Fun geometric fact - because arithmetic mean is that point, sum of squares of differences from which is minimum, if we're in a Eucleadian 2D plane, arithmetic mean of the x coordinates of the points gives us a number, sum of squares of differences of x coordinates of all the points from this number is minimum, and similarly the mean of y coordinates gives us a similar number. Together the arithmetic means of x and y coordinates is in fact the coordinates of the point where both the sums ie of squares of x differences and that of squares of y differences are minimum. Thus for this point the sum of these 2 sums would also be the minimum or this is the point on the plane from which sum of squares of distances (or differences) of all the points is minimum. \n",
    "\n",
    "Thus be it on a line or a plane and likewise in 3D or higher dimensions, arithmetic mean of coordinates of the points gives us the coordinates of the point from which sum of squared distances is minimum (we can also say sum of L2 Norms is minimum).\n",
    "\n",
    "*Note on L1 and L2 norms*: **L1 norm is the sum the absolute values of components of a vector. L2 norm is the square root of the sum of squares of components of a vector**. So what is the vector in the current discussion - the vector is that arrow that starts from the mean/median and goes towards the data point.\n",
    "\n",
    "On a line or 1D, L1 and L2 norm are same and median minimizes the sum of this from all the points but in case of 2D or higher dimensions, median only minimizes the sum of L1 norms ie for 2D or higher dimensions, median of the coordinates of the points gives us the point from which sum of L1 norms or sum of x and y distances over all points is minimized, ie not the sum of true geometric distances or sum of L2 norms is not minimized. The point that does that ie the point that minimizes sum of L2 norms is called the geometric median.\n",
    "\n",
    "**Thus arithmetic mean minimizes the sum of squared L2 norms, whereas geometric median minimizes the sum of L2 norms and the coordinate-wise median minimizes the sum of L1 norms.**\n",
    "\n",
    "The point about geometric median can be ignored as it is a bit unnecessary but it was important to discuss that mean of coordinates of data points gives us the point from which sum of squared distances of data points is minimum and this fact is used when we do K means clustering which is an essential starting point to learn Unsupervised Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the issue of cost function, we can think that just like the mean minimizes the sum of squared differences and zeroes out the sum of differences, we can, for the linear regression problem too try to pick a cost function that involves the sum of squared error terms and then try to minimize this hoping to net out or cancel the differences and most closely represent the data. We divide the sum of squared errors by the number of training examples to keep the cost bounded as it is expected that more the number of examples the greater is the sum of squared errors and dividing by $m$ helps us compare the costs between differently numbered training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that while trying to approximate a bunch of numbers on a number line by a single number, when we chose mean it both minimizes the sum of squared errors and makes the algebraic sum of errors go to zero.\n",
    "In case of finding the staight line fit to the data for linear regression (in a single variable/feature) problem, making the sum of errors to zero alone (which gives the below equation) doesn't give us a unique solution.\n",
    "$$\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})=0$$\n",
    "Above equation has 2 unknowns ($w$ and $b$) and thus we need something more to uniquely find both $w$ and $b$. Thus we need MSE cost as we'll see that minimizing the MSE cost gives both the above equation and the additional info required to find $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq (2) note that $J$ is quadratic in both $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\\frac {\\partial J}{\\partial w} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})x^{(i)}}{m}\\tag{4} \\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})}{m} \\tag{5} \\end{align*}\n",
    "\n",
    "Equating both the partial derivatives to zero we get:\n",
    "\\begin{align*} \\sum_{i=1}^{m} \\left((wx^{(i)} + b-y^{(i)})x^{(i)}\\right) &=0 \\tag{6} \\\\\n",
    "\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})&=0 \\tag{7} \\end{align*}\n",
    "\n",
    "Eq (7) is the one we wrote earlier when we set out to make the sum of error terms zero which we see now is 1 of the results of minimizing the MSE cost function. Together the above 2 equations give us the sufficient information info to solve for 2 unkowns $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector and Matrix equations for Linear Regression\n",
    "For Multiple Variable Linear Regression, vectorially the equations are:\n",
    "\\begin{align*} f^{(i)} &= \\mathbf w \\cdot \\mathbf x^{(i)} + b \\tag{8}\\\\\n",
    "J &= \\sum_i \\frac {(f^{(i)}-y^{(i)})^2}{2m} \\tag{9}\\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\sum_i \\frac{(f^{(i)}-y^{(i)})\\mathbf x^{(i)}}{m} \\tag{10}\\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum_i \\frac{(f^{(i)}-y^{(i)})}{m} \\tag{11} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data set is given as a matrix where every training example is on a separate row, and j<sup>th</sup> column in a matrix represents the j<sup>th</sup> feature in every example, we can write following matrix equations:\n",
    "\\begin{align*} \\mathbf f &= \\mathbf X \\mathbf w + b \\tag{12} \\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\frac {\\mathbf X^T(\\mathbf f -\\mathbf y)}{m} \\tag{13} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts, $\\mathbf w$ and $b$ are combined in a single parameter $ \\boldsymbol {\\theta} $ with b as the first component of $\\boldsymbol{\\theta} $ and the remaining components coming from $\\mathbf w$. Here the feature matrix must be prefixed with a column containing all 1(s). For such a notation, the following holds:\n",
    "$$\\mathbf f = \\mathbf X\\boldsymbol{\\theta}$$\n",
    "Optimizing $J$:\n",
    "\\begin{align*} \\frac {\\partial J}{\\partial \\boldsymbol{\\theta}} &=0 \\\\\n",
    "\\frac {\\mathbf X^T(\\mathbf X\\boldsymbol{\\theta} - \\mathbf y)}{m} &= 0\\\\\n",
    "\\mathbf X^T \\mathbf X\\boldsymbol{\\theta} &= \\mathbf X^T\\mathbf y\\\\\n",
    "\\boldsymbol{\\theta} &= (\\mathbf X^T \\mathbf X)^{-1}\\mathbf X^T\\mathbf y \\tag{14} \\end{align*}\n",
    "\n",
    "Above eq known as **normal equation** gives the exact solution of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and cost plots wrt parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have an analytical solution for Linear Regression, we'll see that an iterative algorithm such as Gradient Descent (with proper techniques like normalization) often would help us get closer to acceptable solutions faster than normal equation. Also this approach works for other types of regression and classification where we dont have an analytical solution. So its recommended to almost never use normal equation method in any real world ML problem ourselves. (Its possible some of the libraries might use normal equation internally) - Given advice is from Andrew Ng. Need some more research to verify some of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the single variable Linear Regression problem, $J$ is quadratic in both $w$ and $b$ (eq (3)) and when plotted forms an upward facing bowl like shape ($w$ and $b$ being in the xy plane and $J$ along the z axis that's pointing upwards). Since $J$ is quadratic in both $w$ and $b$, keeping either of $w$ or $b$ constant and plotting $J$ vs the other produces an upwards facing parabola. Expanding the terms in eq (3) gives:\n",
    "\n",
    "\\begin{align*} J &= \\sum \\frac{(x^{(i)})^2w^2 + b^2 + (y^{(i)})^2 + 2x^{(i)}wb - 2y^{(i)}b - 2x^{(i)}y^{(i)}w} {2m} \\\\\n",
    "\\\\\n",
    "&= \\frac{\\displaystyle{\\left(\\sum({x^{(i)}})^2\\right) w^2 +\\ \\left(2\\sum x^{(i)} \\right) wb + mb^2 - \\left(2\\sum x^{(i)}y^{(i)} \\right)w - \\left(2\\sum y^{(i)} \\right) b + \\sum(y^{(i)})^2}} {2m} \\end{align*}\n",
    "\n",
    "Now since  $\\displaystyle{m\\sum({x^{(i)}})^2 - \\left(\\sum x^{(i)}\\right)^2 > 0}$, for any set of real $x^{(i)}$, above expression when equated to any value would give the equation of an ellipse (in the variables $w$ and $b$) and thus we can say that the contour plots of cost are ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to Gradient Descent, the objective is to find a path from an initial cost towards a direction which leads to the steepest slope or descent. We keep on updating the parameters $w$ and $b$ going in the direction of steepest descent with a learning rate $\\alpha$ till the successive descents stop providing an appreciable cost reduction or till we reach a minima for $J$ which is a function of $w$ and $b$. The minima we reach through a Gradient Descent is a local minima but in case of linear regression it is also a global one.\n",
    "\n",
    "Need to keep $\\alpha$ small enough to not overshoot cost in successive iterations but big enough to not slow down the convergence to the solution.\n",
    "\n",
    "Gradient Descent Algo:\n",
    "\\begin{align*} w &\\to w - \\alpha \\frac {\\partial J}{\\partial w} \\tag{15}\\\\\n",
    "b &\\to b - \\alpha \\frac {\\partial J}{\\partial b} \\tag{16}\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular library for Linear Algebra ie for doing maths on vectors and matrices. Vectors represented as 1d arrays and Matrices as 2d arrays, while NumPy supports higher dimensioned arrays as well. Total number of dimensions (also called **axes** in NumPy terminology) is called the **rank** of the NumPy array. A 2d np array can be thought of as an array of (m) arrays each of which in turn contains (n) scalars. And, we say (m,n) is the **shape** (which is basically a tuple in Python) of this 2d array. Rank here is 2. Thus len of shape is rank. And m is the 1st axis and n is the last axis. When such an np array is printed, it's shown as a grid of m rows and n columns.\n",
    "\n",
    "### Some properties/syntax with examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "shape of a: (5,)\n",
      "rank of a: 1\n",
      "data type of a: <class 'numpy.ndarray'>\n",
      "data type of elements of a: int32\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "print(a)\n",
    "print(f'shape of a: {a.shape}')\n",
    "print(f'rank of a: {len(a.shape)}')\n",
    "print(f'data type of a: {type(a)}')\n",
    "print(f'data type of elements of a: {a.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4.]\n",
      " [5. 6. 7. 8.]]\n",
      "shape of b: (2, 4)\n",
      "rank of b: 2\n",
      "data type of elements of b: float64\n"
     ]
    }
   ],
   "source": [
    "b= np.array([[1.0,2,3,4],[5,6,7,8]])\n",
    "print(b)\n",
    "print(f'shape of b: {b.shape}')\n",
    "print(f'rank of b: {len(b.shape)}')\n",
    "print(f'data type of elements of b: {b.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the elements of np array must be of the same data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "shape of c: ()\n",
      "rank of c: 0\n",
      "data type of elements of c: float64\n",
      "type of c: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "c=np.array(3.0)\n",
    "print(c)\n",
    "print(f'shape of c: {c.shape}')\n",
    "print(f'rank of c: {len(c.shape)}')\n",
    "print(f'data type of elements of c: {c.dtype}')\n",
    "print(f'type of c: {type(c)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*An np array can also have 0 dimensions or rank = 0 but can still contain a scalar value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(3) == c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While checking equality, it checks for value not the data type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emptyArray1 = np.array([])\n",
    "print(emptyArray1.shape)\n",
    "emptyArray1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can have an empty array of shape (0,) with no elements. Contrast this from earlier where we had shape as an empty tuple but had a value (which was 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() missing required argument 'object' (pos 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: array() missing required argument 'object' (pos 0)"
     ]
    }
   ],
   "source": [
    "np.array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it seems we can't have an empty array with no elements with shape also being (). Need to pass atleast something, either an empty list or a scalar value while building np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(2) # 1d array of 2 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,)) #Passing a scalar or a tuple with length 1 as a parameter produces same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "d=np.zeros((3,4,2))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10) # accepts only an integer not a tuple. Produces 1d array of that many elements starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78745649, 0.22815109, 0.04346343, 0.60629973, 0.56631849,\n",
       "       0.68268549, 0.53360991, 0.50686169, 0.55880697, 0.80054397])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random.rand also accepts an integer as a parameter (not a tuple) and produces array of given length with value [0,1). random.random_sample accepts a tuple too as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7583801 , 0.93510172, 0.88793483, 0.48953159, 0.91958823,\n",
       "       0.5794756 , 0.3711545 , 0.63061935, 0.68224789, 0.69938625])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random_sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34719564, 0.66872739],\n",
       "       [0.02564214, 0.12422266],\n",
       "       [0.68309453, 0.55549225],\n",
       "       [0.58875464, 0.0978877 ],\n",
       "       [0.91238643, 0.98325602]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random_sample((5,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like lists in Python we can slice an np array with an identical syntax `a[start:stop:step]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 6. 7. 8.]\n",
      "[5. 6. 7. 8.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(b[1,:]) # from row number 1 pick all columns\n",
    "print(b[1]) # results in same as above\n",
    "print(b[:,1]) # from every row, pick column number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the above produce 1d arrays from 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "print(b[:,1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice diff btw above 2 results. When range is given in the slice it preserves shape, when an explicit number is given, it flattens the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=np.array([[1],[2],[3],[4],[5]])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get above using **reshape**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=e.reshape((5,))\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get back to original by this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape((5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While providing the expected shape parameter in the reshape method, we can put a -1 in exactly 1 of the places and it figures out the right shape automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "\n",
    "Quite elegant functionality of NumPy arrays. Helps to do arithmetic between arrays of different shapes. A simple arithmetic like addition or multiplication of 1 np array with another object should require the other object also to be an np array of exactly same shape because arithmetic is done element wise between these np arrays. But broadcasting relaxes this constraint and expands (think of it as by copying) the axis of length 1 in 1 array to match the size of the corresponding axis in the 2<sup>nd</sup> array or adds more axes in the smaller of the 2 arrays or even scalars so that we get 2 arrays having the same number of axes and with same lengths giving us the same number of elements in both arrays to facilitate the element-wise arithmetic.\n",
    "\n",
    "Only 2 imp rules to remember abt broadcasting:\n",
    "1. It compares shapes from the right most axis of the 2 arrays moving leftward until we reach the end for 1 of the arrays.\\\n",
    "So if a.shape = (5,4,2) \\\n",
    "and b.shape = (4,2) \\\n",
    "Then we can do arithmetic with a and b, because all (the 2) dimensions of b are same as the last 2 dimensions of a. But if b's shape were (3,4,2) then it wouldn't have worked because 3 is not equal to 5.\n",
    "\n",
    "1. Also for a pair of dimensions to be compatible they either have to be same or 1 of those 2 need to be 1.\\\n",
    "So if b.shape were (4,1) then also a and b would be compatible.\\\n",
    "It would have also worked if shape of b had been (2,)\n",
    "\n",
    "Eg where both the arrays in an arithmetic expression are broadcasted: if a.shape = (8,1,6,1) and b.shape = (7,1,5), then the result.shape = (8,7,6,5).\\\n",
    "Some egs in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4],[5,6]])\n",
    "a+3 # array (shape (3,2)) and scalar --> shape (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [5, 6],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= np.array([[1],[2],[3]])\n",
    "a+b # shapes (3,2) and (3,1) --> shape (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [ 9, 11]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([4,5])\n",
    "a+c # shapes (3,2) and (2,) --> shape (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43md\u001b[49m \u001b[38;5;66;03m#shapes (3,2) and (3,) --> error\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (3,) "
     ]
    }
   ],
   "source": [
    "d = np.array([4,5,6])\n",
    "a+d #shapes (3,2) and (3,) --> error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7, 6, 5)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((8,1,6,1))\n",
    "b = np.ones((7,1,5))\n",
    "(a+b).shape #both arrays are broadcasted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "\n",
    "`np.dot(a,b)` If $a$ and $b$ are both 1d arrays it is exactly like a dot product between 2 vectors ($a$ and $b$ must have same shape ie same number of components) ie a sum of product of their respective components or sum product in short. Also either $a$ or $b$ or both of these parameters can be scalars too in which case it behaves like a multiplication of a vector with scalar or just a simple product of 2 scalars (when both are scalars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "dot_product_scalars = np.dot(3,4)\n",
    "print(dot_product_scalars)\n",
    "print(type(dot_product_scalars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "dot_product_0d_arrays = np.dot(np.array(3),np.array(4))\n",
    "print(dot_product_0d_arrays)\n",
    "print(type(dot_product_0d_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "a=np.array([3,4])\n",
    "b=np.array([1.,0])\n",
    "dot_product_vectors = np.dot(a,b)\n",
    "print(dot_product_vectors)\n",
    "print(type(dot_product_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 8]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a,2)) #dot product between vector and scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(a,np.array([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting isn't meant for 'dot', it's meant for simple arithmetic like addition, subtraction , multiplication etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*np.array([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. If $a$ is an N-D array and $b$ is a 1-D array, it is a sum product over the last axis of $a$ and $b$.**\\\n",
    "Above property implies that if $a$ is a 2d array then the result is exactly like a matrix multiplication in maths, as if $b$ were a single column matrix.\n",
    "\n",
    "Thus when we need to multiply a (multi-row, multi-column) matrix with a 'single column' matrix, instead of representing  matrix multiplication between two 2d arrays in NumPy, for simplicity, we can just show it as a dot product of multi-row, multi-column matrix (2d array) with a 'single column' matrix represented as a 1d array (although technically a matrix by definition is always a 2d object whether it contains multiple columns or not) and then interpret the result of the dot also as a 'single column' matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "w:[3 5]\n",
      "X dot w: [13 29 45]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[1,2],[3,4],[5,6]])\n",
    "w=np.array([3,5])\n",
    "print(f'X:\\n{X}')\n",
    "print(f'w:{w}')\n",
    "print(f'X dot w: {np.dot(X,w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "W:\n",
      "[[3]\n",
      " [5]]\n",
      "XW:\n",
      "[[13]\n",
      " [29]\n",
      " [45]]\n"
     ]
    }
   ],
   "source": [
    "W= np.array([[3],[5]])\n",
    "print(f'X:\\n{X}')\n",
    "print(f'W:\\n{W}')\n",
    "print(f'XW:\\n{X@W}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`@` is the operator for matrix multiplication. `a@b` is shorthand for `np.matmul(a,b)`.*\\\n",
    "**2. Incidentally np.dot between two 2d arrays (two matrices) produces the same result as matmul although matmul or `@` is preferred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot:\n",
      "[[13]\n",
      " [29]\n",
      " [45]]\n",
      "matmul:\n",
      "[[13]\n",
      " [29]\n",
      " [45]]\n"
     ]
    }
   ],
   "source": [
    "print(f'dot:\\n{np.dot(X,W)}')\n",
    "print(f'matmul:\\n{np.matmul(X,W)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Actually if both $a$ and $b$ are np arrays of ranks between 1 and 2 or when 1 of the arrays has rank 1 and the other has a rank even greater than 2 then both the operators produce the same result. Matmul doesn't support any of its operands to be scalar or even 0d np array so if operands could possibly be scalars then must use dot instead of matmul*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dot w: [13 29 45]\n",
      "X matmul w: [13 29 45]\n"
     ]
    }
   ],
   "source": [
    "print(f'X dot w: {np.dot(X,w)}')\n",
    "print(f'X matmul w: {np.matmul(X,w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot_product_vectors: 3.0\n",
      "matmul_vectors: 3.0\n"
     ]
    }
   ],
   "source": [
    "a=np.array([3,4])\n",
    "b=np.array([1.,0])\n",
    "print(f'dot_product_vectors: {np.dot(a,b)}')\n",
    "print(f'matmul_vectors: {np.matmul(a,b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m c\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      2\u001b[0m d\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "c=np.array([3])\n",
    "d=np.array(2)\n",
    "np.matmul(c,d) # trying matmul with 0d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "np.matmul(c,2) # trying matmul with scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b**\\\n",
    "1 example of above rule is when we multiply a 1d array with a 2d array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "[3 5]\n",
      "b:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "a dot b:\n",
      "[23 31 39]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([3,5])\n",
    "b=np.array([[1,2,3],[4,5,6]])\n",
    "print(f'a:\\n{a}')\n",
    "print(f'b:\\n{b}')\n",
    "print(f'a dot b:\\n{np.dot(a,b)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result looks similar to a matrix multiplication of a 'single row' matrix with a general (multi row) matrix. Thus although technically a matrix should be represented as a 2d array but when we need to multiply a 'single row' matrix with a general matrix, we can also simply show it as a dot product between a 1d array and 2d array and then interpret the result also as a 'single row' matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a matmul b:\n",
      "[23 31 39]\n"
     ]
    }
   ],
   "source": [
    "print(f'a matmul b:\\n{a@b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also as noted before 3rd property, matmul gives the same result as well for arrays of ranks between 1 and 2.\\\n",
    "Also numerically speaking, we will get the same result (the shape would be different though) if the 1st np array were also a 2d array and there was a proper matrix(2d array) to matrix(2d array) multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matmul b:\n",
      "[[23 31 39]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[3,5]])\n",
    "print(f'A matmul b:\\n{A@b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thus both for dot and matmul, a 1d array as a left argument behaves like a single row matrix while as a right argument it behaves like a single column matrix.** The likeness is not absolute as the shape is different though had it actually been a 2d array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference in results between matmul and dot\n",
    "If either of $a$ or $b$ has a rank greater than 2 then matmul assumes it to be an array (of shape sans the last 2 indices of the original array's shape) of matrices. Each of the operands now can be thought of as either an array of matrices or just a matrix. The case here is similar to doing arithmetic like addition or multiplication between different arrays or between scalars and arrays and thus matmul applies rules of broadcasting to resolve the difference if one operand has a different shape of array of matrices than the other.\\\n",
    "For eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3,4,2)->(3,newaxis,newaxis) (5,2,6)->(5,newaxis,newaxis)  and requested shape (4,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones([\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,4,2)->(3,newaxis,newaxis) (5,2,6)->(5,newaxis,newaxis)  and requested shape (4,6)"
     ]
    }
   ],
   "source": [
    "a = np.ones([3,4,2])\n",
    "b = np.ones([5,2,6])\n",
    "np.matmul(a,b).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of $a$ as an array of 3 elements each of which is a matrix and $b$ as an array of 5 elements each of which is also a matrix. Now the shapes of matrices in these 2 arrays agree for a successful matrix multiplication ie shapes of matrices in $a$ which is (4,2) agree with shapes of matrices in $b$ which is (2,6) for a proper matrix multiplication but just like we can't multiply an array of shape (3,) with an array of shape (5,), it will throw an error. However if $b$'s shape were (5,3,2,6) then it would have worked because arrays of shapes (3,) and (5,3) can have arithmetic done between according to broadcast rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4, 6)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ = np.ones([5,3,2,6])\n",
    "np.matmul(a,b_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas for dot, there actually is no broadcasting, and there actually is no requirement for a successful dot between 2 arrays of ranks greater than 2 other than that the last index of shape of 1st array ($a$) must match with the 2nd last index of shape of 2nd array ($b$). The final shape of the result is all the indices of shape of $a$ except the last one followed by all indices of shape of $b$ except the 2nd last one.\\\n",
    "For eg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape:(3, 4, 2)\n",
      "b.shape:(5, 2, 6)\n",
      "b_.shape:(5, 3, 2, 6)\n",
      "np.dot(a,b).shape:(3, 4, 5, 6)\n",
      "np.dot(a,b_).shape:(3, 4, 5, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'a.shape:{a.shape}')\n",
    "print(f'b.shape:{b.shape}')\n",
    "print(f'b_.shape:{b_.shape}')\n",
    "print(f'np.dot(a,b).shape:{np.dot(a,b).shape}')\n",
    "print(f'np.dot(a,b_).shape:{np.dot(a,b_).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model implementation\n",
    "\n",
    "From [Vector Equations for Linear Regression](#vector-and-matrix-equations-for-linear-regression), eq (12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_output(X,w,b):\n",
    "    \"\"\"\n",
    "    Returns Linear Regression Model prediction: 1d array (m,)\n",
    "    \"\"\"\n",
    "    f=np.dot(X,w)+b\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq (9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_cost(X,y,w,b):\n",
    "    \"\"\"\n",
    "    Returns Linear Regression Cost (scalar) without regularization\n",
    "    \"\"\"\n",
    "    m=X.shape[0]\n",
    "    f=linear_model_output(X,w,b)\n",
    "    J=np.sum((f-y)**2)/(2*m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq (13) and (11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_derivatives(X,y,w,b):\n",
    "    \"\"\"\n",
    "    Returns Linear Regression Derivatives (tuple of ndarray and scalar) without regularization\n",
    "    \"\"\"\n",
    "    m=X.shape[0]\n",
    "    f=linear_model_output(X,w,b)\n",
    "    dJdw=(np.dot(X.T,f-y))/m\n",
    "    dJdb=(np.sum(f-y))/m\n",
    "    return dJdw,dJdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Gradient Descent](#gradient-descent-and-cost-plots-wrt-parameters) eq (15) and (16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_gradient_descent(X,y,w,b,num_iter,a=0.03,cost_fn=linear_model_cost, derivative_fn=linear_model_derivatives):\n",
    "    \"\"\"\n",
    "    Returns final parameters w, b and lists of histories of cost and the parameters w and b\n",
    "    \"\"\"\n",
    "    w_hist = [w]\n",
    "    b_hist = [b]\n",
    "    J=cost_fn(X,y,w,b)\n",
    "    J_hist = [J]\n",
    "    for i in range(num_iter):\n",
    "        dJdw,dJdb = derivative_fn(X,y,w,b)\n",
    "        w=w-a*dJdw\n",
    "        b=b-a*dJdb\n",
    "        J = cost_fn(X,y,w,b)\n",
    "        w_hist.append(w)\n",
    "        b_hist.append(b)\n",
    "        J_hist.append(J)\n",
    "        if J_hist[-1]>J_hist[-2]:\n",
    "            print(f'Cost increased from {J_hist[-2]} to {J_hist[-1]}!')\n",
    "            break\n",
    "    return w,b,J_hist,w_hist,b_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Vector Equations for Linear Regression](#vector-and-matrix-equations-for-linear-regression), eq (14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_sol_normal_eq(X,y):\n",
    "    \"\"\"\n",
    "    Returns solution (1d array) of Linear Regression by normal equation method. 1st component is 'b'\n",
    "    \"\"\"\n",
    "    X=np.c_[np.ones(len(X)),X]\n",
    "    theta = np.linalg.inv((X.T@X))@X.T@y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Variable example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0, 500.0])\n",
    "w_in,b_in = 0.,0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ ie learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost increased from 6.623907248959792e-24 to 6.6990320506858e-24!\n",
      "Final parameters: 199.99999999999292, 100.00000000001148\n"
     ]
    }
   ],
   "source": [
    "w,b,J_hist,w_hist,b_hist  = linear_model_gradient_descent(x_train,y_train,w_in,b_in,10000,0.1)\n",
    "print(f'Final parameters: {w}, {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:     0,  J:      85000.0,  w:                  0.0,  b:                  0.0\n",
      "iteration:   383,  J:    0.0539361,  w:      199.36069409245,  b:     101.034418687625\n",
      "iteration:   766,  J:  0.000197785,  w:     199.961286239582,  b:     100.062640180189\n",
      "iteration:  1149,  J:  7.25281e-07,  w:     199.997655652438,  b:     100.003793234037\n",
      "iteration:  1532,  J:  2.65962e-09,  w:     199.999858035865,  b:     100.000229702795\n",
      "iteration:  1915,  J:  9.75287e-12,  w:     199.999991403231,  b:     100.000013909865\n",
      "iteration:  2298,  J:   3.5764e-14,  w:     199.999999479415,  b:     100.000000842325\n",
      "iteration:  2681,  J:  1.31148e-16,  w:     199.999999968475,  b:     100.000000051008\n",
      "iteration:  3064,  J:  4.80915e-19,  w:     199.999999998091,  b:     100.000000003089\n",
      "iteration:  3447,  J:  1.76404e-21,  w:     199.999999999884,  b:     100.000000000187\n",
      "iteration:  3831,  J:  6.69903e-24,  w:     199.999999999993,  b:     100.000000000011\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(11,len(J_hist))):\n",
    "    iter = i if len(J_hist)<11 else ((len(J_hist)-1)*i)//10\n",
    "    print(f'iteration: {iter:5},  J: {J_hist[iter]:12.6},  w: {w_hist[iter]:20.15},  b: {b_hist[iter]:20.15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost seemed to increase because of precision error as the cost's value is extremely small in the order of -24. Took around 3830 iterations to converge to solution with $\\alpha$ = 0.1 beyond which precision errors restrict us going further.\n",
    "\n",
    "$\\alpha$ = 0.03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: 199.9999999967735, 100.00000000522058\n",
      "iteration:     0,  J:      85000.0,  w:                  0.0,  b:                  0.0\n",
      "iteration:  1000,  J:     0.183909,  w:     198.819489411282,  b:     101.910106256624\n",
      "iteration:  2000,  J:   0.00229952,  w:     199.867995816037,  b:     100.213587256309\n",
      "iteration:  3000,  J:  2.87523e-05,  w:     199.985239349185,  b:     100.023883234715\n",
      "iteration:  4000,  J:  3.59508e-07,  w:     199.998349470404,  b:     100.002670612987\n",
      "iteration:  5000,  J:  4.49515e-09,  w:     199.999815438494,  b:      100.00029862679\n",
      "iteration:  6000,  J:  5.62056e-11,  w:     199.999979362412,  b:     100.000033392319\n",
      "iteration:  7000,  J:  7.02774e-13,  w:     199.999997692314,  b:     100.000003733915\n",
      "iteration:  8000,  J:  8.78721e-15,  w:     199.999999741955,  b:     100.000000417525\n",
      "iteration:  9000,  J:  1.09873e-16,  w:     199.999999971146,  b:     100.000000046688\n",
      "iteration: 10000,  J:  1.37376e-18,  w:     199.999999996774,  b:     100.000000005221\n"
     ]
    }
   ],
   "source": [
    "w,b,J_hist,w_hist,b_hist  = linear_model_gradient_descent(x_train,y_train,w_in,b_in,10000,0.03)\n",
    "print(f'Final parameters: {w}, {b}')\n",
    "for i in range(min(11,len(J_hist))):\n",
    "    iter = i if len(J_hist)<11 else ((len(J_hist)-1)*i)//10\n",
    "    print(f'iteration: {iter:5},  J: {J_hist[iter]:12.6},  w: {w_hist[iter]:20.15},  b: {b_hist[iter]:20.15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished all 10K iterations without the value of cost going in the order of -24. Learning rate at 0.03 is acceptable although less optimum than 0.1 for this case.\n",
    "\n",
    "Try $\\alpha$ = 0.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost increased from 85000.0 to 257800.0!\n",
      "Final parameters: 520.0, 320.0\n",
      "iteration:     0,  J:      85000.0,  w:                  0.0,  b:                  0.0\n",
      "iteration:     1,  J:    2.578e+05,  w:                520.0,  b:                320.0\n"
     ]
    }
   ],
   "source": [
    "w,b,J_hist,w_hist,b_hist  = linear_model_gradient_descent(x_train,y_train,w_in,b_in,10000,0.8)\n",
    "print(f'Final parameters: {w}, {b}')\n",
    "for i in range(min(11,len(J_hist))):\n",
    "    iter = i if len(J_hist)<11 else ((len(J_hist)-1)*i)//10\n",
    "    print(f'iteration: {iter:5},  J: {J_hist[iter]:12.6},  w: {w_hist[iter]:20.15},  b: {b_hist[iter]:20.15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts to diverge with a really large $\\alpha$ such as 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 200.])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model_sol_normal_eq(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Variable Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in, b_in = np.zeros_like(X_train[0]),0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [ 0.21184806  0.01985303 -0.06605406 -0.37678891], -0.012482833827459511\n",
      "iteration:     0,  J:      49518.0,  w: [0 0 0 0],  b:                  0.0\n",
      "iteration:  1000,  J:      690.745,  w: [ 0.20325556  0.00243834 -0.00670331 -0.03785026],  b: -0.00130522000248555\n",
      "iteration:  2000,  J:      684.735,  w: [ 0.20431599  0.00440327 -0.01351177 -0.07967923],  b: -0.00269389425556291\n",
      "iteration:  3000,  J:      679.033,  w: [ 0.20534833  0.00636025 -0.02026341 -0.12040048],  b: -0.00404358625715419\n",
      "iteration:  4000,  J:      673.623,  w: [ 0.20635333  0.00830948 -0.02695971 -0.16004319],  b: -0.00535532247830027\n",
      "iteration:  5000,  J:       668.49,  w: [ 0.2073317   0.01025116 -0.03360213 -0.19863575],  b: -0.00663010236578055\n",
      "iteration:  6000,  J:       663.62,  w: [ 0.20828414  0.01218549 -0.04019208 -0.23620581],  b: -0.00786889905358973\n",
      "iteration:  7000,  J:      658.998,  w: [ 0.20921135  0.01411266 -0.04673093 -0.27278028],  b: -0.00907266005568347\n",
      "iteration:  8000,  J:      654.612,  w: [ 0.21011397  0.01603285 -0.05322002 -0.30838539],  b:  -0.0102423079404858\n",
      "iteration:  9000,  J:       650.45,  w: [ 0.21099267  0.01794625 -0.05966064 -0.34304665],  b:  -0.0113787409876383\n",
      "iteration: 10000,  J:      646.499,  w: [ 0.21184806  0.01985303 -0.06605406 -0.37678891],  b:  -0.0124828338274595\n"
     ]
    }
   ],
   "source": [
    "w,b,J_hist,w_hist,b_hist  = linear_model_gradient_descent(X_train,y_train,w_in,b_in,10000,0.0000003)\n",
    "print(f'Final parameters: {w}, {b}')\n",
    "for i in range(min(11,len(J_hist))):\n",
    "    iter = i if len(J_hist)<11 else ((len(J_hist)-1)*i)//10\n",
    "    print(f'iteration: {iter:5},  J: {J_hist[iter]:12.6},  w: {w_hist[iter]},  b: {b_hist[iter]:20.15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try again with these new parameters, for another round of 10K iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [ 0.21924778  0.03859224 -0.1276486  -0.66868711], -0.021921758941350195\n",
      "iteration:     0,  J:      646.499,  w: [ 0.21184806  0.01985303 -0.06605406 -0.37678891],  b:  -0.0124828338274595\n",
      "iteration:  1000,  J:       642.75,  w: [ 0.21268077  0.02175336 -0.07240153 -0.40963637],  b:  -0.0135554380635682\n",
      "iteration:  2000,  J:      639.192,  w: [ 0.21349139  0.02364741 -0.07870423 -0.44161258],  b:  -0.0145973828791155\n",
      "iteration:  3000,  J:      635.814,  w: [ 0.2142805   0.02553533 -0.08496335 -0.47274048],  b:  -0.0156094756270555\n",
      "iteration:  4000,  J:      632.607,  w: [ 0.21504867  0.0274173  -0.09118001 -0.50304241],  b:  -0.0165925024048768\n",
      "iteration:  5000,  J:      629.563,  w: [ 0.21579645  0.02929346 -0.09735533 -0.53254012],  b:  -0.0175472286142021\n",
      "iteration:  6000,  J:      626.673,  w: [ 0.21652437  0.03116396 -0.10349039 -0.56125477],  b:  -0.0184743995056552\n",
      "iteration:  7000,  J:      623.928,  w: [ 0.21723296  0.03302895 -0.10958623 -0.58920698],  b:  -0.0193747407093832\n",
      "iteration:  8000,  J:      621.322,  w: [ 0.21792274  0.03488857 -0.11564389 -0.61641683],  b:  -0.0202489587516111\n",
      "iteration:  9000,  J:      618.847,  w: [ 0.21859418  0.03674295 -0.12166436 -0.64290386],  b:   -0.021097741557598\n",
      "iteration: 10000,  J:      616.497,  w: [ 0.21924778  0.03859224 -0.1276486  -0.66868711],  b:  -0.0219217589413502\n"
     ]
    }
   ],
   "source": [
    "w_in, b_in = w,b\n",
    "w,b,J_hist,w_hist,b_hist  = linear_model_gradient_descent(X_train,y_train,w_in,b_in,10000,0.0000003)\n",
    "print(f'Final parameters: {w}, {b}')\n",
    "for i in range(min(11,len(J_hist))):\n",
    "    iter = i if len(J_hist)<11 else ((len(J_hist)-1)*i)//10\n",
    "    print(f'iteration: {iter:5},  J: {J_hist[iter]:12.6},  w: {w_hist[iter]},  b: {b_hist[iter]:20.15}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is still decreasing at an extremely low rate and partly because learning rate is very low, increasing the learning rate makes the process diverge. Lets look at actual solution by normal equation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[372], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlinear_model_sol_normal_eq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[337], line 6\u001b[0m, in \u001b[0;36mlinear_model_sol_normal_eq\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mReturns solution (1d array) of Linear Regression by normal equation method. 1st component is 'b'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mc_[np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X)),X]\n\u001b[1;32m----> 6\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@X\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;129m@X\u001b[39m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@y\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m theta\n",
      "File \u001b[1;32mc:\\Users\\ayana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\ayana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "linear_model_sol_normal_eq(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't seem to solve this via normal equation method as we can see easily that we dont have enough examples. (4 variables but only 3 examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Math\n",
    "\n",
    "|Symbol| Meaning|\n",
    "|--|--|\n",
    "|$x$ | feature|\n",
    "|$y$ | target|\n",
    "|$x^{(i)}$ | i<sup>th</sup> training example in case of Single Variable Linear Regression|\n",
    "|$\\hat y$ | estimated value of target|\n",
    "|$m$|No. of training examples|\n",
    "|$n$|No. of features in a single training example|\n",
    "|$x^{(i)}_j$ | j<sup>th</sup> feature of i<sup>th</sup> training example for Multiple Variable Linear Regression|\n",
    "|$\\mathbf x^{(i)}$ | i<sup>th</sup> training example for Multiple Variable Linear Regression represented as a vector|\n",
    "\n",
    "Note on notations: Capital bold face used to denote Matrix (or 2d arrays) and small bold face symbol to denote a (column) vector (or 1d array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Model Representation** (sometimes called a hypothesis function):\n",
    "\n",
    "For a single featured training set:\n",
    "\\begin{align*} f(x^{(i)}) &= \\hat {y^{(i)}} \\\\\n",
    "f_{wb}(x^{(i)}) &= wx^{(i)} + b \\tag{1} \\end{align*}\n",
    "\n",
    "**Mean Squared Error (MSE)** Cost\n",
    "\\begin{align*}J &= \\sum_{i=1}^{m} \\frac{(f(x^{(i)})-y^{(i)})^2}{2m} \\tag{2} \\\\\n",
    "J_{wb} &= \\sum_{i=1}^{m} \\frac{(wx^{(i)} + b-y^{(i)})^2}{2m} \\tag{3} \\end{align*}\n",
    "\n",
    "To find $w$ and $b$, and thus the estimator function we must minimize cost ( $J$ ) wrt $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuition for Mean Squared Error Cost function consider this - to approximate a bunch of numbers by a single number we often cite the data's average or arithmetic mean. \n",
    "\n",
    "**Mean is the point from which the arithmetic sum of differences from all the numbers is zero.**\n",
    "\n",
    "Let $\\bar x$ denote the mean of a set of scalar $x^{(i)}$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} (x^{(i)}-\\bar x) = \\sum_{i=1}^{m}x^{(i)} - m\\bar x = \\sum_{i=1}^{m}x^{(i)}- \\sum_{i=1}^{m}x^{(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we were to find a number, lets say $\\mu$, sum of squares of differences from which of all the numbers in a dataset, is to be minimized, we'd see that that number, is in fact, the mean.\n",
    "To minimize $$ \\sum_{i=1}^{m} (x^{(i)}-\\mu)^2$$\n",
    "lets take the derivative of the above term wrt $\\mu$ and equate it to zero.\n",
    "\\begin{align*}\\dfrac{d}{d \\mu} \\left(\\sum_{i=1}^{m} (x^{(i)}-\\mu)^2\\right)=0 \\\\\n",
    "\\sum_{i=1}^{m} \\left(2(x^{(i)}-\\mu) \\cdot \\frac{d(x^{(i)}-\\mu)}{d \\mu}\\right) = 0 \\\\\n",
    "-2 \\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\Rightarrow \\mu = \\bar x\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Thus (arithmetic) mean is the point, sum of squares of differences from which of all the numbers in a dataset, is minimum.**\n",
    "\n",
    "**Related fact - median is the point, sum of absolute differences (or distances) from which of all the numbers in a dataset is minimum.**\n",
    "\n",
    "Fun geometric fact - because arithmetic mean is that point sum of squares of differences from which is minimum, if we're in a Eucleadian 2D plane, arithmetic mean of the x coordinates of the points gives us a number, sum of squares of differences of x coordinates of all the points from this number is minimum, and similarly the mean of y coordinates gives us a similar number. Together the arithmetic means of x and y coordinates is in fact the cooridnates of the point where both the sums ie of squares of x differences and that of squares of y differences are minimum. Thus for this point the sum of these 2 sums would also be the minimum or this is the point on the plane from which sum of squares of distances (or differences) of all the points is minimum. \n",
    "\n",
    "Thus be it on a line or a plane and likewise in 3D or higher dimensions, arithmetic mean of coordinates of the points gives us the coordinates of the point from which sum of squared L2 Norms is minimum.\n",
    "\n",
    "On a line or 1D, L1 and L2 norm are same and median minimizes the sum of this from all the points but in case of 2D or higher dimensions, median only minimizes the L1 norm ie for 2D or higher dimensions, median of the coordinates of the points gives us the point from which L1 norm or sum of x and y distances over all points is minimized, ie not the sum of true geometric distances or L2 norm is minimized. The point that does that ie the point that minimizes L2 norm is called geometric median.\n",
    "\n",
    "**Thus arithmetic mean minimizes the sum of squared L2 norms, whereas geometric median minimizes the sum of L2 norms and the cooridnate-wise median minimizes the sum of L1 norms.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the issue of cost function, we can think that just like the mean minimizes the sum of squared differences and zeroes out the sum of differences, by trying to best fit the data on a straight line ie while finding the equation of a line for the linear regression problem we try to pick a cost function that involves the sum of squared error terms and then try to minimize this hoping to net out or cancel the differences and most closely represent the data. We divide the sum of squared errors by the number of training examples to keep the cost bounded as it is expected that more the number of examples the greater is the sum of squared errors and dividing by $m$ helps us compare the costs between differently numbered training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that while trying to approximate a bunch of numbers on a number line by a single number, when we chose mean it both minimizes the sum of squared errors and makes the algebraic sum of errors go to zero.\n",
    "In case of finding the staight line fit to the data for linear regression (in a single variable/feature) problem, making the sum of errors to zero alone (which gives the below equation) doesn't give us a unique solution.\n",
    "$$\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})=0$$\n",
    "Above equation has 2 unknowns ($w$ and $b$) and thus we need something more to uniquely find both $w$ and $b$. Thus we need MSE cost as we'll see that minimizing the MSE cost gives both the above equation and the additional info required to find $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq (2) note that $J$ is quadratic in both $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\\frac {\\partial J}{\\partial w} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})x^{(i)}}{m}\\tag{4} \\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})}{m} \\tag{5} \\end{align*}\n",
    "\n",
    "Equating both the partial derivatives to zero we get:\n",
    "\\begin{align*} \\sum_{i=1}^{m} \\left((wx^{(i)} + b-y^{(i)})x^{(i)}\\right) &=0 \\tag{6} \\\\\n",
    "\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})&=0 \\tag{7} \\end{align*}\n",
    "\n",
    "Eq (7) is the one we wrote earlier when we set out to make the sum of error terms zero which we see now is 1 of the results of minimizing the MSE cost function. Together the above 2 equations give us the sufficient information info to solve for 2 unkowns $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Multiple Variable Linear Regression**, vectorially the equations are:\n",
    "\\begin{align*} f^{(i)} &= \\mathbf w \\cdot \\mathbf x^{(i)} + b \\tag{8}\\\\\n",
    "J &= \\sum_i \\frac {(f^{(i)}-y^{(i)})^2}{2m} \\tag{9}\\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\sum_i \\frac{(f^{(i)}-y^{(i)})\\mathbf x^{(i)}}{m} \\tag{10}\\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum_i \\frac{(f^{(i)}-y^{(i)})}{m} \\tag{11} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data set is given as a matrix where every training example is on a separate row with a particular feature along that particular column, we can write following matrix equations:\n",
    "\\begin{align*} \\mathbf f &= \\mathbf X \\mathbf w + b \\tag{12} \\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\frac {\\mathbf X^T(\\mathbf f -\\mathbf y)}{m} \\tag{13} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts, $\\mathbf w$ and $b$ are combined in a single parameter $ \\boldsymbol {\\theta} $ with b as the first component of $\\boldsymbol{\\theta} $ and the remaining components coming from $\\mathbf w$. Here the feature matrix must be prefixed with a column containing all 1(s). For such a notation, the following holds:\n",
    "$$\\mathbf f = \\mathbf X\\boldsymbol{\\theta}$$\n",
    "Optimizing $J$:\n",
    "\\begin{align*} \\frac {\\partial J}{\\partial \\boldsymbol{\\theta}} &=0 \\\\\n",
    "\\frac {\\mathbf X^T(\\mathbf X\\boldsymbol{\\theta} - \\mathbf y)}{m} &= 0\\\\\n",
    "\\mathbf X^T \\mathbf X\\boldsymbol{\\theta} &= \\mathbf X^T\\mathbf y\\\\\n",
    "\\boldsymbol{\\theta} &= (\\mathbf X^T \\mathbf X)^{-1}\\mathbf X^T\\mathbf y \\tag{14} \\end{align*}\n",
    "\n",
    "Above eq known as **normal equation** gives the exact solution of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and cost plots wrt parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have an analytical solution for Linear Regression, we'll see that an iterative algorithm such as Gradient Descent (with proper techniques like normalization) often would help us get closer to acceptable solutions faster than normal equation. Also this approach works for other types of regression and classification where we dont have an analytical solution. So its recommended to almost never use normal equation method in any real world ML problem ourselves. (Its possible some of the libraries might use normal equation internally) - Given advice is from Andrew Ng. Need some more research to verify some of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the single variable Linear Regression problem, $J$ is quadratic in both $w$ and $b$ (eq (3)) and when plotted forms an upward facing bowl like shape ($w$ and $b$ being in the xy plane and $J$ along the z axis that's pointing upwards). Since $J$ is quadratic in both $w$ and $b$, keeping either of $w$ or $b$ constant and plotting $J$ vs the other produces an upwards facing parabola. Expanding the terms in eq (3) gives:\n",
    "\n",
    "\\begin{align*} J &= \\sum \\frac{(x^{(i)})^2w^2 + b^2 + (y^{(i)})^2 + 2x^{(i)}wb - 2y^{(i)}b - 2x^{(i)}y^{(i)}w} {2m} \\\\\n",
    "\\\\\n",
    "&= \\frac{\\displaystyle{\\left(\\sum({x^{(i)}})^2\\right) w^2 +\\ \\left(2\\sum x^{(i)} \\right) wb + mb^2 - \\left(2\\sum x^{(i)}y^{(i)} \\right)w - \\left(2\\sum y^{(i)} \\right) b + \\sum(y^{(i)})^2}} {2m} \\end{align*}\n",
    "\n",
    "Now since \n",
    "$$m\\sum({x^{(i)}})^2 - \\left(\\sum x^{(i)}\\right)^2 > 0 \\quad \\text{, for any set of real}  \\ x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above expression when equated to any value would give the equation of an ellipse (in the variables $w$ and $b$) and thus we can say that the contour plots of cost are ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to Gradient Descent, the objective is to find a path from an initial cost towards a direction which leads to the steepest slope or descent. We keep on updating the parameters $w$ and $b$ going in the direction of steepest descent with a learning rate $\\alpha$ till the successive descents doesn't lead to an appreciable cost reduction.\n",
    "\n",
    "Need to keep $\\alpha$ small enough to not overshoot cost in successive iterations but big enough to not slow down the convergence to the solution.\n",
    "\n",
    "Gradient Descent Algo:\n",
    "\\begin{align*} w &\\to w - \\alpha \\frac {\\partial J}{\\partial w} \\\\\n",
    "b &\\to b - \\alpha \\frac {\\partial J}{\\partial b} \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

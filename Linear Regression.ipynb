{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Math\n",
    "\n",
    "|Symbol| Meaning|\n",
    "|--|--|\n",
    "|$x$ | feature|\n",
    "|$y$ | target|\n",
    "|$x^{(i)}$ | i<sup>th</sup> training example in case of Single Variable Linear Regression|\n",
    "|$\\hat y$ | estimated value of target|\n",
    "|$m$|No. of training examples|\n",
    "|$n$|No. of features in a single training example|\n",
    "|$x^{(i)}_j$ | j<sup>th</sup> feature of i<sup>th</sup> training example for Multiple Variable Linear Regression|\n",
    "\n",
    "**Model Representation** (sometimes called a hypothesis function):\n",
    "\n",
    "For a single featured training set:\n",
    "\\begin{align*} f(x^{(i)}) &= \\hat {y^{(i)}} \\\\\n",
    "f_{wb}(x^{(i)}) &= wx^{(i)} + b \\tag{1} \\end{align*}\n",
    "\n",
    "**Mean Squared Error (MSE)** Cost\n",
    "\\begin{align*}J &= \\sum_{i=1}^{m} \\frac{(f(x^{(i)})-y^{(i)})^2}{2m} \\tag{2} \\\\\n",
    "J_{wb} &= \\sum_{i=1}^{m} \\frac{(wx^{(i)} + b-y^{(i)})^2}{2m} \\tag{3} \\end{align*}\n",
    "\n",
    "To find $w$ and $b$, and thus the estimator function we must minimize cost ( $J$ ) wrt $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuition for Mean Squared Error Cost function consider this - to approximate a bunch of numbers by a single number we often cite the data's average or arithmetic mean. \n",
    "\n",
    "**Mean is the point from which the arithmetic sum of differences from all the numbers is zero.**\n",
    "\n",
    "Let $\\bar x$ denote the mean of a set of scalar $x^{(i)}$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} (x^{(i)}-\\bar x) = \\sum_{i=1}^{m}x^{(i)} - m\\bar x = \\sum_{i=1}^{m}x^{(i)}- \\sum_{i=1}^{m}x^{(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we were to find a number, lets say $\\mu$, sum of squares of differences from which of all the numbers in a dataset, is to be minimized, we'd see that that number, is in fact, the mean.\n",
    "To minimize $$ \\sum_{i=1}^{m} (x^{(i)}-\\mu)^2$$\n",
    "lets take the derivative of the above term wrt $\\mu$ and equate it to zero.\n",
    "\\begin{align*}\\dfrac{d}{d \\mu} \\left(\\sum_{i=1}^{m} (x^{(i)}-\\mu)^2\\right)=0 \\\\\n",
    "\\sum_{i=1}^{m} \\left(2(x^{(i)}-\\mu) \\cdot \\frac{d(x^{(i)}-\\mu)}{d \\mu}\\right) = 0 \\\\\n",
    "-2 \\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\sum_{i=1}^{m} (x^{(i)}-\\mu) = 0 \\\\\n",
    "\\Rightarrow \\mu = \\bar x\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Thus (arithmetic) mean is the point, sum of squares of differences from which of all the numbers in a dataset, is minimum.**\n",
    "\n",
    "**Related fact - median is the point, sum of absolute differences (or distances) from which of all the numbers in a dataset is minimum.**\n",
    "\n",
    "Fun geometric fact - because arithmetic mean is that point sum of squares of differences from which is minimum, if we're in a Eucleadian 2D plane, arithmetic mean of the x coordinates of the points gives us a number, sum of squares of differences of x coordinates of all the points from this number is minimum, and similarly the mean of y coordinates gives us a similar number. Together the arithmetic means of x and y coordinates is in fact the cooridnates of the point where both the sums ie of squares of x differences and that of squares of y differences are minimum. Thus for this point the sum of these 2 sums would also be the minimum or this is the point on the plane from which sum of squares of distances (or differences) of all the points is minimum. \n",
    "\n",
    "Thus be it on a line or a plane and likewise in 3D or higher dimensions, arithmetic mean of coordinates of the points gives us the coordinates of the point from which sum of squared L2 Norms is minimum.\n",
    "\n",
    "On a line or 1D, L1 and L2 norm are same and median minimizes the sum of this from all the points but in case of 2D or higher dimensions, median only minimizes the L1 norm ie for 2D or higher dimensions, median of the coordinates of the points gives us the point from which L1 norm or sum of x and y distances over all points is minimized, ie not the sum of true geometric distances or L2 norm is minimized. The point that does that ie the point that minimizes L2 norm is called geometric median.\n",
    "\n",
    "**Thus arithmetic mean minimizes the sum of squared L2 norms, whereas geometric median minimizes the sum of L2 norms and the cooridnate-wise median minimizes the sum of L1 norms.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the issue of cost function, we can think that just like the mean minimizes the sum of squared differences and zeroes out the sum of differences, by trying to best fit the data on a straight line ie while finding the equation of a line for the linear regression problem we try to pick a cost function that involves the sum of squared error terms and then try to minimize this hoping to net out or cancel the differences and most closely represent the data. We divide the sum of squared errors by the number of training examples to keep the cost bounded as it is expected that more the number of examples the greater is the sum of squared errors and dividing by $m$ helps us compare the costs between differently numbered training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that while trying to approximate a bunch of numbers on a number line by a single number, when we chose mean it both minimizes the sum of squared errors and makes the algebraic sum of errors go to zero.\n",
    "In case of finding the staight line fit to the data for linear regression (in a single variable/feature) problem, making the sum of errors to zero alone (which gives the below equation) doesn't give us a unique solution.\n",
    "$$\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})=0$$\n",
    "Above equation has 2 unknowns ($w$ and $b$) and thus we need something more to uniquely find both $w$ and $b$. Thus we need MSE cost as we'll see that minimizing the MSE cost gives both the above equation and the additional info required to find $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From eq (2) note that $J$ is quadratic in both $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\\frac {\\partial J}{\\partial w} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})x^{(i)}}{m}\\tag{4} \\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum_{i=1}^{m} \\frac {(wx^{(i)} + b-y^{(i)})}{m} \\tag{5} \\end{align*}\n",
    "\n",
    "Equating both the partial derivatives to zero we get:\n",
    "\\begin{align*} \\sum_{i=1}^{m} \\left((wx^{(i)} + b-y^{(i)})x^{(i)}\\right) &=0 \\tag{6} \\\\\n",
    "\\sum_{i=1}^{m} (wx^{(i)} + b-y^{(i)})&=0 \\tag{7} \\end{align*}\n",
    "\n",
    "Eq (7) is the one we wrote earlier when we set out to make the sum of error terms zero which we see now is 1 of the results of minimizing the MSE cost function. Together the above 2 equations give us the sufficient information info to solve for 2 unkowns $w$ and $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Multiple Variable Linear Regression**, vectorially the equations are (ignoring the superscript $i$ for i<sup>th</sup> training example):\n",
    "\\begin{align*} f &= \\mathbf w \\cdot \\mathbf x + b \\tag{8}\\\\\n",
    "J &= \\sum \\frac {(f-y)^2}{2m} \\tag{9}\\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\sum \\frac{(f-y)\\mathbf x}{m} \\tag{10}\\\\\n",
    "\\frac {\\partial J}{\\partial b} &= \\sum \\frac{(f-y)}{m} \\tag{11} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data set is given as a matrix where every training example is on a separate row with a particular feature along that particular column:\n",
    "\\begin{align*} \\mathbf f &= \\mathbf X \\mathbf w + b \\tag{12} \\\\\n",
    "\\frac {\\partial J}{\\partial \\mathbf w} &= \\mathbf X^T(\\mathbf f -\\mathbf y) \\tag{13} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
